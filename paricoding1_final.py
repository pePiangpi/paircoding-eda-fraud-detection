# -*- coding: utf-8 -*-
"""paricoding1_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PiZbWPGOAj6eq_00diB-OlQ2u17vuQY8
"""

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

# Install dependencies as needed:
# pip install kagglehub[pandas-datasets]
import kagglehub
from kagglehub import KaggleDatasetAdapter

# Set the path to the file you'd like to load
file_path = "creditcard.csv"

# Load the latest version
df = kagglehub.dataset_load(
  KaggleDatasetAdapter.PANDAS,
  "mlg-ulb/creditcardfraud",
  file_path,
  # Provide any additional arguments like
  # sql_query or pandas_kwargs. See the
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)
print("first 5 records:", df.head())

print("\n--- EDA 1: Missing Values ---")
print(f"Max missing values in any column: {df.isnull().sum().max()}")

print("\n--- EDA 2: Duplicate Records ---")
duplicate_count = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_count}")

"""Reason: Before building any model, it’s essential to check for missing or null values because they can lead to biased or incomplete analysis.

Purpose: Ensures data quality and reliability — confirming the dataset (credit card fraud data) has no missing entries that might distort results.
"""

# EDA #2: Analyze Class Imbalance (Crucial for Fraud Detection)
print("\n--- EDA 2: Class Imbalance ---")
class_counts = df['Class'].value_counts()
print(class_counts)
print(f"Fraudulent percentage: {class_counts[1]/len(df) * 100:.4f}%")

# Visualize the Imbalance
plt.figure(figsize=(6, 4))
sns.countplot(x='Class', data=df)
plt.title('Transaction Class Distribution (0=Non-Fraud, 1=Fraud)')
plt.ylabel('Count (Log Scale)')
plt.yscale('log')
plt.show()

"""Reason: In fraud detection, there are usually very few fraud cases compared to normal ones.

Purpose: This step helps us see how uneven the data is. Knowing this is important because it guides how we train our model — for example, we might need to balance the data or use special methods to detect rare fraud cases.
"""

print("\n--- EDA 3: Transaction Amount Comparison ---")
f_df = df[df['Class'] == 1]['Amount']
nf_df = df[df['Class'] == 0]['Amount']

print("Fraudulent Amount Stats:")
print(f_df.describe())
print("\nNon-Fraudulent Amount Stats:")
print(nf_df.describe())

# Visualize the distributions
plt.figure(figsize=(10, 6))
sns.histplot(f_df, bins=50, color='red', label='Fraud', stat='density', kde=True)
sns.histplot(nf_df, bins=50, color='blue', alpha=0.5, label='Non-Fraud', stat='density', kde=True)
plt.title('Transaction Amount Distribution')
plt.xlim(0, 1000)
plt.legend()
plt.show()

"""Reason: A count plot helps us easily see the difference between fraud and normal transactions.

Purpose: Using a log scale makes it easier to notice small differences, even when fraud cases are much fewer than normal ones.
"""

# ku myout EDA

print("\n--- EDA 4: Transaction Time Analysis ---")

df['Hour'] = (df['Time'] / 3600) % 24

plt.figure(figsize=(10, 6))
sns.kdeplot(df[df['Class'] == 1]['Hour'], label='Fraud', color='red', fill=True)
sns.kdeplot(df[df['Class'] == 0]['Hour'], label='Non-Fraud', color='blue', alpha=0.5)
plt.xticks(np.arange(0, 24, 2))
plt.title('Transaction Density by Hour of Day')
plt.legend()
plt.show()

"""Reason: This is the first EDA step to make sure the data is loaded correctly and to see what it looks like — such as the columns, data types, and a few sample rows.

Purpose: It helps us find what needs to be prepared or cleaned before analysis, like scaling numbers or changing text into numeric form.
"""

# EDA #5: Correlation with the Target
print("\n--- EDA 5: Correlation with Target 'Class' ---")

corr_matrix = df.drop(columns=['Hour']).corr()
class_corr = corr_matrix['Class'].sort_values(ascending=False)

print("Top 5 Positive and Negative Correlations with Fraud (Class):")
# Show top 3 positive (except Class itself) and top 3 negative correlations
print(class_corr.head(4))
print(class_corr.tail(3))

# Check the distribution of all the features
df.hist(figsize=(15,12))
plt.title("Features Distribution")
plt.show()

# Calculate and visualize correlation matrix
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=False)
plt.title('Correlation Matrix')
plt.show()